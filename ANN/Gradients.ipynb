{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Step1: Perform standard imports\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Step 2. Create a tensor with requires_grad set to True\r\n",
    "# This sets up computational tracking on the tensor.\r\n",
    "x = torch.tensor(2.0, requires_grad=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Step 3. Define a function\r\n",
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\r\n",
    "\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since $y$ was created as a result of an operation, it has an associated gradient function accessible as <tt>y.grad_fn</tt><br>\r\n",
    "The calculation of $y$ is done as:<br>\r\n",
    "\r\n",
    "$\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$\r\n",
    "\r\n",
    "This is the value of $y$ when $x=2$.\r\n",
    "\r\n",
    "#### Step 4. Backprop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "y.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Step5: Display the resulting gradient\r\n",
    "print(x.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(93.)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that <tt>x.grad</tt> is an attribute of tensor $x$, so we don't use parentheses. The computation is the result of<br>\r\n",
    "\r\n",
    "$\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$\r\n",
    "\r\n",
    "This is the slope of the polynomial at the point $(2,63)$.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Back-propagation on multiple steps\r\n",
    "Now let's do something more complex, involving layers $y$ and $z$ between $x$ and our output layer $out$.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 1. Create a tensor\r\n",
    "x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\r\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [3., 2., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#2 Create the first layer with y =3x+2\r\n",
    "y= 3*x + 2\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 5.,  8., 11.],\n",
      "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Create the second layer with $z = 2y^2$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "z = 2*y**2\r\n",
    "print(z)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 4. Set the output to be the matrix mean\r\n",
    "out = z.mean()\r\n",
    "print(out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(140., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 5. Now perform back-propagation to find the gradient of x w.r.t out\r\n",
    "out.backward()\r\n",
    "print(x.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[10., 16., 22.],\n",
      "        [22., 16., 10.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turn off tracking\r\n",
    "There may be times when we don't want or need to track the computational history.\r\n",
    "\r\n",
    "You can reset a tensor's <tt>requires_grad</tt> attribute in-place using `.requires_grad_(True)` (or False) as needed.\r\n",
    "\r\n",
    "When performing evaluations, it's often helpful to wrap a set of operations in `with torch.no_grad():`\r\n",
    "\r\n",
    "A less-used method is to run `.detach()` on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('pytorchenv': conda)"
  },
  "interpreter": {
   "hash": "63df7b24d2537257e4e7a7bcd8a0da0caf8171d07815a73aa47b4288cce197db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}